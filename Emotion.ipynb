{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOTION RECOGNITION THROUGH SPEECH (A naive attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project was neither to solve the problem of emotion recognition nor to build a highly accurate model but rather to show that this problem is solvable. This was a naive attempt to do so and may contain a few mistakes here and there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a lot of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import cross_val_score as cv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression  as LR\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.preprocessing import MinMaxScaler as scaler\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier as XGB\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import stft\n",
    "import pickle\n",
    "scale = scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for target labels\n",
    "emotions = {\n",
    " './wav_hs\\\\happy': 1,\n",
    " './wav_hs\\\\sadness': 0}\n",
    "files = glob.glob(\"./wav_hs/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features to train our predictive model\n",
    "# Size of feature vector - 112\n",
    "# Only mfccs, its mean and its derivative are used as features\n",
    "\n",
    "data = []\n",
    "target = []\n",
    "for file in files:\n",
    "    audios = glob.glob(file+\"/*\")\n",
    "    for i in audios:\n",
    "        sample_rate, X = wav.read(i)\n",
    "        libceps = librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=13)\n",
    "        libceps = np.transpose(libceps)\n",
    "        mfcc_delta = librosa.feature.delta(libceps)\n",
    "        num_ceps = len(libceps)\n",
    "        \n",
    "        #mfcc mean\n",
    "        mfcc_mean = np.mean(libceps[int(num_ceps/10):int(num_ceps*9/10)], axis =0)\n",
    "        mfcc_max = libceps.max(axis=0)\n",
    "        mfcc_min = libceps.min(axis=0)\n",
    "        mfcc_var = libceps.var(axis=0)\n",
    "        \n",
    "        #mfcc_delta\n",
    "        mfcc_delta_mean = np.mean(mfcc_delta[int(num_ceps/10):int(num_ceps*9/10)], axis =0)\n",
    "        mfcc_delta_max = mfcc_delta.max(axis=0)\n",
    "        mfcc_delta_min = mfcc_delta.min(axis=0)\n",
    "        mfcc_delta_var = mfcc_delta.var(axis=0)\n",
    "        \n",
    "        #mfcc_mean\n",
    "        mfcc_mean_mean = np.mean(mfcc_mean)\n",
    "        mfcc_mean_max = mfcc_mean.max()\n",
    "        mfcc_mean_min = mfcc_mean.min()\n",
    "        mfcc_mean_var = mfcc_mean.var()\n",
    "        \n",
    "        #mfcc_delta_mean\n",
    "        mfcc_delta_mean_mean = np.mean(mfcc_delta_mean)\n",
    "        mfcc_delta_mean_max = mfcc_delta_mean.max()\n",
    "        mfcc_delta_mean_min = mfcc_delta_mean.min()\n",
    "        mfcc_delta_mean_var = mfcc_delta_mean.var()\n",
    "        feature = np.hstack((mfcc_mean,mfcc_max,mfcc_min,mfcc_var,mfcc_delta_mean,mfcc_delta_max,mfcc_delta_min,mfcc_delta_var,\n",
    "                            mfcc_mean_mean,mfcc_mean_max,mfcc_mean_min,mfcc_mean_var,\n",
    "                            mfcc_delta_mean_mean,mfcc_delta_mean_max,mfcc_delta_mean_min,mfcc_delta_mean_var))\n",
    "        data.append(feature)\n",
    "        target.append(emotions[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "target = np.array(target)\n",
    "print(\"Size of Feature Vector: \",len(data[0]),\"\\nNumber of 'Happy' Examples: \",len(target[target==1]),\n",
    "      \"\\nNumber of 'Sad' Examples: \",len(target[target==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data,target,train_size=0.6,random_state=0)\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness of the training dataset\n",
    "from scipy.stats import skew\n",
    "sk = 0\n",
    "for i in range(112):\n",
    "    sk += abs(skew(X_train[:,i]))\n",
    "print(sk/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X_train, y_train)\n",
    "dval = lgb.Dataset(X_test, y_test)\n",
    "params = {'num_leaves' : 256,\n",
    "         'learning_rate':0.03,\n",
    "         'metric':'accuracy',\n",
    "         'objective':'binary',\n",
    "         'early_stopping_round': 40,\n",
    "         'max_depth':8,\n",
    "         'bagging_fraction':0.5,\n",
    "         'feature_fraction':0.6,\n",
    "         'bagging_seed':2017,\n",
    "         'feature_fraction_seed':2017,\n",
    "         'verbose' : 1,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.train(params, dtrain,num_boost_round=500,valid_sets=(dtrain,dval),verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y_test,clf.predict(X_test)))\n",
    "print(len(y_test[clf.predict(X_test)>=0.5])/len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building our predictive model with other algos\n",
    "\n",
    "#clf = LR(penalty='l2',C=0.01,max_iter=100)\n",
    "#clf = SVC(kernel='linear',C=0.01,probability=True,random_state=0)\n",
    "#clf = RFC(n_estimators=50,max_depth=12,random_state=0)\n",
    "#clf = MLP(hidden_layer_sizes=[1,10,100],solver='adam',random_state=0)\n",
    "#clf = KNN(n_neighbors=5)\n",
    "scores = cv(clf,data,target,cv=10) # for parameter Optimization\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save = open(\"./trained.pickle\",\"wb\")\n",
    "pickle.dump(clf,save)\n",
    "save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
